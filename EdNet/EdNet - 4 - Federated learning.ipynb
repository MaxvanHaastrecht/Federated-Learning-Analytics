{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f0ac85",
   "metadata": {},
   "source": [
    "This notebook uses the CSVs created with the EdNet - 1 - Feature engineering notebook. Make sure to run the feature engineering code before continuing with this notebook. Additionally, this notebook uses the non-standard library [PyTorch](https://pytorch.org/), which you may need to install before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b513e",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df5c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as torch_utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c8b45",
   "metadata": {},
   "source": [
    "### Define function to create final question features based on users in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f192466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_features(x):\n",
    "    x_new = x.sort_values('timestamp')\n",
    "    \n",
    "    q_accuracy = []\n",
    "    part_accuracy = []\n",
    "    user_excess_correct = []\n",
    "    \n",
    "    q_dict = {}\n",
    "    part_dict = {}\n",
    "    user_dict = {}\n",
    "    for part, q_id, u_id, correct in zip(x_new['part'], x_new['question_id'], x_new['user_id'], x_new['correct_response']):\n",
    "        # Calculate excess correct first to avoid contamination\n",
    "        excess_correct = 0\n",
    "        if q_id in q_dict:\n",
    "            avg_q_acc = q_dict[q_id]['n_correct']/q_dict[q_id]['n_ans']\n",
    "            excess_correct = correct - avg_q_acc\n",
    "        elif part in part_dict:\n",
    "            avg_p_acc = part_dict[part]['n_correct']/part_dict[part]['n_ans']\n",
    "            excess_correct = correct - avg_p_acc\n",
    "        else:\n",
    "            excess_correct = correct - 0.5# default\n",
    "        \n",
    "        if q_id in q_dict:\n",
    "            q_accuracy.append(q_dict[q_id]['n_correct']/q_dict[q_id]['n_ans'])\n",
    "            q_dict[q_id]['n_ans'] += 1\n",
    "            q_dict[q_id]['n_correct'] += correct\n",
    "        else:\n",
    "            q_accuracy.append(np.nan)\n",
    "            q_dict[q_id] = {'n_ans': 1, 'n_correct': correct}\n",
    "            \n",
    "        if part in part_dict:\n",
    "            part_accuracy.append(part_dict[part]['n_correct']/part_dict[part]['n_ans'])\n",
    "            part_dict[part]['n_ans'] += 1\n",
    "            part_dict[part]['n_correct'] += correct\n",
    "        else:\n",
    "            part_accuracy.append(np.nan)\n",
    "            part_dict[part] = {'n_ans': 1, 'n_correct': correct}\n",
    "            \n",
    "        if u_id in user_dict:\n",
    "            avg_excess_correct = user_dict[u_id]['sum_excess_correct'] / user_dict[u_id]['n_ans']\n",
    "            user_excess_correct.append(avg_excess_correct)\n",
    "            \n",
    "            user_dict[u_id]['n_ans'] += 1\n",
    "            user_dict[u_id]['sum_excess_correct'] += excess_correct\n",
    "        else:\n",
    "            user_excess_correct.append(np.nan)\n",
    "            user_dict[u_id] = {'n_ans': 1, 'sum_excess_correct': excess_correct}\n",
    "            \n",
    "    x_new['q_acc'] = q_accuracy\n",
    "    x_new['part_acc'] = part_accuracy\n",
    "    x_new['usr_excess_correct'] = user_excess_correct\n",
    "    \n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72910223",
   "metadata": {},
   "source": [
    "### Define functions to split data into train-test and into local clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_y_from_df(df):\n",
    "    X = df.drop(columns=['timestamp', 'solving_id', 'question_id', 'elapsed_time',\n",
    "                                 'user_id', 'part', 'correct_response']).fillna(0).to_numpy()\n",
    "    y = df['correct_response'].to_numpy().ravel()\n",
    "    \n",
    "    X_torch = torch.tensor(X, dtype=torch.float32)\n",
    "    y_torch = torch.tensor(y, dtype=torch.int64)\n",
    "    \n",
    "    return X_torch, y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_holdout_and_clients(df, n_clients, seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    uids = list(df['user_id'].unique())\n",
    "    np.random.shuffle(uids)\n",
    "    \n",
    "    n_holdout_uids = int(len(uids) * 0.2)\n",
    "    holdout_uids = uids[0:n_holdout_uids]\n",
    "    local_uids = uids[n_holdout_uids:]\n",
    "    \n",
    "    df_holdout = create_question_features(df.loc[df['user_id'].isin(holdout_uids)])\n",
    "    X_holdout, y_holdout = X_y_from_df(df_holdout)\n",
    "    \n",
    "    clients_uids = np.array_split(local_uids, n_clients)\n",
    "    \n",
    "    clients = []\n",
    "    for client_uids in clients_uids:\n",
    "        df_client = create_question_features(df.loc[df['user_id'].isin(client_uids)])\n",
    "        \n",
    "        X_client, y_client = X_y_from_df(df_client)\n",
    "\n",
    "        clients.append((X_client, y_client))\n",
    "\n",
    "    return X_holdout, y_holdout, clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aee8b5",
   "metadata": {},
   "source": [
    "### Define federated learning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46306b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_layer    = nn.Linear(in_dim,16)\n",
    "        self.hidden_layer1  = nn.Linear(16,8)\n",
    "        self.output_layer   = nn.Linear(8,out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out =  self.relu(self.input_layer(x))\n",
    "        out =  self.relu(self.hidden_layer1(out))\n",
    "        out =  self.output_layer(out)\n",
    "        return out\n",
    "    \n",
    "def federated_averaging(global_model, local_models, num_clients, client_sizes):\n",
    "    dataset_size = sum(client_sizes)\n",
    "    \n",
    "    for param_global, params_local in zip(global_model.parameters(), zip(*[model.parameters() for model in local_models])):\n",
    "        weighted_sum = torch.zeros_like(param_global.data)\n",
    "        for client_params, client_size in zip(params_local, client_sizes):\n",
    "            client_weight = client_size / dataset_size\n",
    "            weighted_sum += client_weight * client_params.data\n",
    "        \n",
    "        param_global.data = weighted_sum\n",
    "        \n",
    "def train_local_model(model, dataloader, optimizer, loss_fn, epochs, device):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(data)\n",
    "            loss = loss_fn(prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "def validate_local_model(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            prediction = model(data)\n",
    "            total_loss += loss_fn(prediction, target).item()\n",
    "            _, predicted_labels = torch.max(prediction, 1)\n",
    "            correct_predictions += (predicted_labels == target).sum().item()\n",
    "            total_samples += len(target)\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_learning(clients, input_dim, output_dim, loss_fn, lr, optim_str, num_clients, num_rounds, num_epochs, batch_size):\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    global_model = NeuralNetwork(input_dim, output_dim).to(device)\n",
    "    \n",
    "    client_sizes = [len(client) for client in clients]\n",
    "    for r in range(num_rounds):\n",
    "        local_models = []\n",
    "        local_optimizers = []\n",
    "        local_dataloaders = []\n",
    "        \n",
    "        for i in range(num_clients):\n",
    "            local_model = NeuralNetwork(input_dim, output_dim).to(device)\n",
    "            local_model.load_state_dict(global_model.state_dict())\n",
    "            local_optimizer = optim.SGD(local_model.parameters(), lr=lr)\n",
    "            if optim_str == 'adam':\n",
    "                local_optimizer = optim.Adam(local_model.parameters(), lr=lr)\n",
    "            \n",
    "            X, y = clients[i]\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            local_dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X, y),\n",
    "                                                           batch_size=batch_size, shuffle=True)\n",
    "            train_local_model(local_model, local_dataloader, local_optimizer, loss_fn, num_epochs, device)\n",
    "            \n",
    "            local_models.append(local_model)\n",
    "            local_optimizers.append(local_optimizer)\n",
    "            local_dataloaders.append(local_dataloader)\n",
    "            \n",
    "        federated_averaging(global_model, local_models, num_clients, client_sizes)\n",
    "        \n",
    "        if r % 5 == 0:\n",
    "            for i in range(num_clients):\n",
    "                X, y = clients[i]\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                local_dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X, y),\n",
    "                                                               batch_size=batch_size, shuffle=False)\n",
    "                val_loss, val_accuracy = validate_local_model(local_models[i], local_dataloader, loss_fn)\n",
    "                print(f\"Client {i+1} - Round {r + 1}/{num_rounds}, Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}\")\n",
    "            \n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f53a0",
   "metadata": {},
   "source": [
    "### Read data and train federated learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc252a14",
   "metadata": {},
   "source": [
    "For this example we use N_CLIENTS = 10. Results for other numbers of local clients can easily be obtained by changing the value of N_CLIENTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ednet_features_10000_users.csv')\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr = 0.02\n",
    "optimizer = 'adam'\n",
    "\n",
    "N_CLIENTS = 10\n",
    "N_ROUNDS = 20\n",
    "N_EPOCHS = 2\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "acc_fed, f1_fed, auc_fed = [], [], []\n",
    "for i in range(10):\n",
    "    X_holdout, y_holdout, torch_clients = split_into_holdout_and_clients(df, N_CLIENTS, seed = i)\n",
    "    \n",
    "    input_dim = X_holdout.shape[1]\n",
    "    output_dim = len(np.unique(y_holdout))\n",
    "    \n",
    "    global_model = federated_learning(torch_clients, input_dim, output_dim, loss_fn, lr, optimizer, N_CLIENTS, N_ROUNDS, N_EPOCHS, BATCH_SIZE)\n",
    "    \n",
    "    global_model.eval()\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    with torch.no_grad():\n",
    "        predictions = global_model(X_holdout.to(device))\n",
    "        \n",
    "    _, y_pred = torch.max(predictions, dim=1)\n",
    "    y_probs = F.softmax(predictions, dim=1)[:, 1]\n",
    "    \n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    y_probs = y_probs.cpu().numpy()\n",
    "    \n",
    "    acc_fed.append(accuracy_score(y_holdout, y_pred))\n",
    "    f1_fed.append(f1_score(y_holdout, y_pred))\n",
    "    auc_fed.append(roc_auc_score(y_holdout, y_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348e261",
   "metadata": {},
   "source": [
    "### Store results as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'acc': acc_fed, 'f1': f1_fed, 'auc': auc_fed})\n",
    "\n",
    "df.to_csv('ednet_federated_10000users_10clients.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:swarm]",
   "language": "python",
   "name": "conda-env-swarm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
